<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
	<!--#include virtual="/header.html" -->
        <title>Cluster Labs - The Home of Pacemaker</title>
        <meta name="description" content="">
    </head>
    <body>
	<!--#include virtual="/banner.html" -->	

	<section id="main">
	  <!--#include virtual="/quickstart-common.html" -->	

	  <h1>Ubuntu</h1>
	  <p>
	    Ubuntu appears to have switched to Corosync 2 for it's LTS releases.
	  </p>
	  <p>
	    We use aptitude to install pacemaker and some other
	    necessary packages we will need:
	  </p>
	  <p class="command">
[ALL] # aptitude install pacemaker corosync fence-agents
	  </p>

	  <h2>Configure Cluster Membership and Messaging</h2>
	  <p>
	    Since the pcs tool from RHEL does not exist on Ubuntu, we
	    well create the corosync configuration file on both machines
	    manually:
          </p>
         <p class="command">
[ALL] # cat <<EOF > /etc/corosync/corosync.conf
totem {
    version: 2
    secauth: off
    cluster_name: pacemaker1
    transport: udpu
}

nodelist {
    node {
        ring0_addr: node1
        nodeid: 101
    }
    node {
        ring0_addr: node2
        nodeid: 102
    }
}

quorum {
   provider: corosync_votequorum
   two_node: 1
   wait_for_all: 1
   last_man_standing: 1
   auto_tie_breaker: 0
}

EOF
          </p>

	  <h2>Start the Cluster</h2>
	  <p>
	    On each machine, run:
	  </p>
	  <p class="command">
[ALL] # service pacemaker start
	  </p>

	  <h2>Set Cluster Options</h2>
	  <p>
	    With so many devices and possible topologies, it is nearly
	    impossible to include Fencing in a document like this.
	    For now we will disable it.
	  </p>
	  <p class="command">
[ONE] # crm configure property stonith-enabled=false
	  </p>
	  <p>
	    One of the most common ways to deploy Pacemaker is in a
	    2-node configuration.  However quorum as a concept makes
	    no sense in this scenario (because you only have it when
	    more than half the nodes are available), so we'll disable
	    it too.
	  </p>
	  <p class="command">
[ONE] # crm configure property no-quorum-policy=ignore
	  </p>
	  <p>
	    For demonstration purposes, we will force the cluster to
	    move services after a single failure:
	  </p>
	  <p class="command">
[ONE] # crm configure property migration-threshold=1
	  </p>

	  <h2>Add a Resource</h2>
	  <p>
	    Lets add a cluster service, we'll choose one doesn't
	    require any configuration and works everywhere to make
	    things easy.  Here's the command:
	  </p>
	  <p class="command">
[ONE] # crm configure primitive my_first_svc ocf:pacemaker:Dummy op monitor interval=120s
	  </p>
	  <p>
	    "<strong>my_first_svc</strong>" is the name the service
	    will be known as.
	  </p>
	  <p>
	    "<strong>ocf:pacemaker:Dummy</strong>" tells Pacemaker
	    which script to use
	    (<a href="https://github.com/ClusterLabs/pacemaker/blob/master/extra/resources/Dummy">Dummy</a>
	    - an agent that's useful as a template and for guides like
	    this one), which namespace it is in (pacemaker) and what
	    standard it conforms to
	    (<a href="/doc/en-US/Pacemaker/1.1-crmsh/html/Pacemaker_Explained/s-resource-supported.html#_open_cluster_framework">OCF</a>).
	  </p>
	  <p>
	    "<strong>op monitor interval=120s</strong>" tells Pacemaker to
	    check the health of this service every 2 minutes by
	    calling the agent's <strong>monitor</strong> action.
	  </p>
	  <p>
	    You should now be able to see the service running using:
	  </p>
          <p class="command">
[ONE] # crm_mon -1
	  </p>

	  <h2>Simulate a Service Failure</h2>
	  <p>
	    We can simulate an error by telling the service stop
	    directly (without telling the cluster):
	  </p>
	  <p class="command">
[ONE] # crm_resource --resource my_first_svc --force-stop
	  </p>
	  <p>
	    If you now run <strong>crm_mon</strong> in interactive
	    mode (the default), you should see (within the monitor
	    interval - 2 minutes) the cluster notice
	    that <strong>my_first_svc</strong> failed and move it to
	    another node.
	  </p>
	  <h2>Next Steps</h2>
	  <p>
	    <ul>
	      <li>Configure <a href="/doc/en-US/Pacemaker/1.1-pcs/html/Clusters_from_Scratch/ch09.html">Fencing</a></li>
	      <li>Add more services - see <a href="/doc/en-US/Pacemaker/1.1-crmsh/html/Clusters_from_Scratch/ch06.html">Clusters from Scratch</a> for examples of how to add IP address, Apache and DRBD to a cluster</li>
	      <li>Learn how to make services <a href="/doc/en-US/Pacemaker/1.1-crmsh/html/Clusters_from_Scratch/_specifying_a_preferred_location.html">prefer a specific host</a></li>
	      <li>Learn how to make services <a href="/doc/en-US/Pacemaker/1.1-crmsh/html/Clusters_from_Scratch/_ensuring_resources_run_on_the_same_host.html">run on the same host</a></li>
	      <li>Learn how to make services <a href="/doc/en-US/Pacemaker/1.1-crmsh/html/Clusters_from_Scratch/_controlling_resource_start_stop_ordering.html">start and stop</a> in a specific order</li>
	      <li>Find out what else Pacemaker can do - see <a href="/doc/en-US/Pacemaker/1.1-crmsh/html/Pacemaker_Explained/index.html">Pacemaker Explained</a> for an comprehensive list of concepts and options</li>
	    </ul>
	  </p>

	</section>	

	<!--#include virtual="/footer.html" -->
    </body>
</html>
